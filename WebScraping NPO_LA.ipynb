{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "url0 = \"https://deepsweep.com/animal-welfare-nonprofits-los-angeles/\"\n",
    "url1 = \"https://deepsweep.com/best-nonprofits-in-los-angeles-2019-so-far/\"\n",
    "url2 = \"https://deepsweep.com/non-profit-funding-los-angeles-list/\"\n",
    "url3 = \"https://deepsweep.com/list-of-homelessness-nonprofits-in-los-angeles/\"\n",
    "url4 = \"https://deepsweep.com/list-of-environmental-nonprofits-in-los-angeles/\"\n",
    "url5 = \"https://deepsweep.com/list-of-disability-nonprofits-in-los-angeles/\"\n",
    "url6 = \"https://deepsweep.com/list-of-museum-nonprofits-in-los-angeles/\"\n",
    "url7 = \"https://deepsweep.com/age-friendly-nonprofits-in-los-angeles/\"\n",
    "url8 = \"https://deepsweep.com/list-of-boys-only-private-schools-los-angeles/\"\n",
    "url9 = \"https://deepsweep.com/top-youth-education-nonprofits-in-los-angeles/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "response0 = requests.get(url0)    \n",
    "data0 = response0.text\n",
    "soup0 = BeautifulSoup(data0)\n",
    "\n",
    "arr0=[]\n",
    "\n",
    "for item0 in soup0.find_all(attrs={'class':'static-pages content-text'}):\n",
    "    for link0 in item0.find_all('a'):\n",
    "        arr0.append(link0.get('href'))\n",
    "        \n",
    "arr1=[]\n",
    "\n",
    "for item0 in soup0.find_all(attrs={'class':'static-pages content-text'}):\n",
    "    for link0 in item0.find_all('a'):\n",
    "        arr1.append(link0.text)\n",
    "        \n",
    "d = {'Name':arr1,'URL':arr0}\n",
    "df = pd.DataFrame(d, columns=['Name','URL'])\n",
    "df.to_csv (r'C:\\Users\\Vishv\\Desktop\\sheet1.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "response1 = requests.get(url1)    \n",
    "data1 = response1.text\n",
    "soup1 = BeautifulSoup(data1)\n",
    "\n",
    "arr2=[]\n",
    "\n",
    "for item1 in soup1.find_all(attrs={'class':'static-pages content-text'}):\n",
    "    for link1 in item1.find_all('a'):\n",
    "        arr2.append(link1.get('href'))\n",
    "\n",
    "arr3=[]\n",
    "\n",
    "for item1 in soup1.find_all(attrs={'class':'static-pages content-text'}):\n",
    "    for link1 in item1.find_all('a'):\n",
    "        arr3.append(link1.text)\n",
    "\n",
    "d1 = {'Name':arr3,'URL':arr2}\n",
    "df1 = pd.DataFrame(d1, columns=['Name','URL'])\n",
    "df1.to_csv (r'C:\\Users\\Vishv\\Desktop\\sheet2.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "response2 = requests.get(url2)    \n",
    "data2 = response2.text\n",
    "soup2 = BeautifulSoup(data2)\n",
    "\n",
    "arr4=[]\n",
    "\n",
    "for item2 in soup2.find_all(attrs={'class':'static-pages content-text'}):\n",
    "    for link2 in item2.find_all('a'):\n",
    "        arr4.append(link2.get('href'))\n",
    "\n",
    "arr5=[]\n",
    "\n",
    "for item2 in soup2.find_all(attrs={'class':'static-pages content-text'}):\n",
    "    for link2 in item2.find_all('a'):\n",
    "        arr5.append(link2.text)\n",
    "\n",
    "d2 = {'Name':arr5,'URL':arr4}\n",
    "df2 = pd.DataFrame(d2, columns=['Name','URL'])\n",
    "df2.to_csv (r'C:\\Users\\Vishv\\Desktop\\sheet3.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "response3 = requests.get(url3)    \n",
    "data3 = response3.text\n",
    "soup3 = BeautifulSoup(data3)\n",
    "\n",
    "arr6=[]\n",
    "\n",
    "for item3 in soup3.find_all(attrs={'class':'static-pages content-text'}):\n",
    "    for link3 in item3.find_all('a'):\n",
    "        arr6.append(link3.get('href'))\n",
    "\n",
    "arr7=[]\n",
    "\n",
    "for item3 in soup3.find_all(attrs={'class':'static-pages content-text'}):\n",
    "    for link3 in item3.find_all('a'):\n",
    "        arr7.append(link3.text)\n",
    "\n",
    "d3 = {'Name':arr7,'URL':arr6}\n",
    "df3 = pd.DataFrame(d3, columns=['Name','URL'])\n",
    "df3.to_csv (r'C:\\Users\\Vishv\\Desktop\\sheet4.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "response4 = requests.get(url4)    \n",
    "data4 = response4.text\n",
    "soup4 = BeautifulSoup(data4)\n",
    "\n",
    "arr8=[]\n",
    "\n",
    "for item4 in soup4.find_all(attrs={'class':'static-pages content-text'}):\n",
    "    for link4 in item4.find_all('a'):\n",
    "        arr8.append(link4.get('href'))\n",
    "\n",
    "arr9=[]\n",
    "\n",
    "for item4 in soup4.find_all(attrs={'class':'static-pages content-text'}):\n",
    "    for link4 in item4.find_all('a'):\n",
    "        arr9.append(link4.text)\n",
    "\n",
    "d4 = {'Name':arr9,'URL':arr8}\n",
    "df4 = pd.DataFrame(d4, columns=['Name','URL'])\n",
    "df4.to_csv (r'C:\\Users\\Vishv\\Desktop\\sheet5.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "response5 = requests.get(url5)    \n",
    "data5 = response5.text\n",
    "soup5 = BeautifulSoup(data5)\n",
    "\n",
    "arr10=[]\n",
    "\n",
    "for item5 in soup5.find_all(attrs={'class':'static-pages content-text'}):\n",
    "    for link5 in item5.find_all('a'):\n",
    "        arr10.append(link5.get('href'))\n",
    "\n",
    "arr11=[]\n",
    "\n",
    "for item5 in soup5.find_all(attrs={'class':'static-pages content-text'}):\n",
    "    for link5 in item5.find_all('a'):\n",
    "        arr11.append(link5.text)\n",
    "\n",
    "d5 = {'Name':arr11,'URL':arr10}\n",
    "df5 = pd.DataFrame(d5, columns=['Name','URL'])\n",
    "df5.to_csv (r'C:\\Users\\Vishv\\Desktop\\sheet6.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "response6 = requests.get(url6)    \n",
    "data6 = response6.text\n",
    "soup6 = BeautifulSoup(data6)\n",
    "\n",
    "arr12=[]\n",
    "\n",
    "for item6 in soup6.find_all(attrs={'class':'static-pages content-text'}):\n",
    "    for link6 in item6.find_all('a'):\n",
    "        arr12.append(link6.get('href'))\n",
    "\n",
    "arr13=[]\n",
    "\n",
    "for item6 in soup6.find_all(attrs={'class':'static-pages content-text'}):\n",
    "    for link6 in item6.find_all('a'):\n",
    "        arr13.append(link6.text)\n",
    "\n",
    "d6 = {'Name':arr13,'URL':arr12}\n",
    "df6 = pd.DataFrame(d6, columns=['Name','URL'])\n",
    "df6.to_csv (r'C:\\Users\\Vishv\\Desktop\\sheet7.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "response7 = requests.get(url7)    \n",
    "data7 = response7.text\n",
    "soup7 = BeautifulSoup(data7)\n",
    "\n",
    "arr14=[]\n",
    "\n",
    "for item7 in soup7.find_all(attrs={'class':'static-pages content-text'}):\n",
    "    for link7 in item7.find_all('a'):\n",
    "        arr14.append(link7.get('href'))\n",
    "\n",
    "arr15=[]\n",
    "\n",
    "for item7 in soup7.find_all(attrs={'class':'static-pages content-text'}):\n",
    "    for link7 in item7.find_all('a'):\n",
    "        arr15.append(link7.text)\n",
    "\n",
    "d7 = {'Name':arr15,'URL':arr14}\n",
    "df7 = pd.DataFrame(d7, columns=['Name','URL'])\n",
    "df7.to_csv (r'C:\\Users\\Vishv\\Desktop\\sheet8.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "response8 = requests.get(url8)    \n",
    "data8 = response8.text\n",
    "soup8 = BeautifulSoup(data8)\n",
    "\n",
    "arr16=[]\n",
    "\n",
    "for item8 in soup8.find_all(attrs={'class':'static-pages content-text'}):\n",
    "    for link8 in item8.find_all('a'):\n",
    "        arr16.append(link8.get('href'))\n",
    "\n",
    "arr17=[]\n",
    "\n",
    "for item8 in soup8.find_all(attrs={'class':'static-pages content-text'}):\n",
    "    for link8 in item8.find_all('a'):\n",
    "        arr17.append(link8.text)\n",
    "\n",
    "d8 = {'Name':arr17,'URL':arr16}\n",
    "df8 = pd.DataFrame(d8, columns=['Name','URL'])\n",
    "df8.to_csv (r'C:\\Users\\Vishv\\Desktop\\sheet9.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "response9 = requests.get(url9)    \n",
    "data9 = response9.text\n",
    "soup9 = BeautifulSoup(data9)\n",
    "\n",
    "arr18=[]\n",
    "\n",
    "for item9 in soup9.find_all(attrs={'class':'static-pages content-text'}):\n",
    "    for link9 in item9.find_all('a'):\n",
    "        arr18.append(link9.get('href'))\n",
    "\n",
    "arr19=[]\n",
    "\n",
    "for item9 in soup9.find_all(attrs={'class':'static-pages content-text'}):\n",
    "    for link9 in item9.find_all('a'):\n",
    "        arr19.append(link9.text)\n",
    "\n",
    "d9 = {'Name':arr19,'URL':arr18}\n",
    "df9 = pd.DataFrame(d9, columns=['Name','URL'])\n",
    "df9.to_csv (r'C:\\Users\\Vishv\\Desktop\\sheet10.csv', index = False, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
